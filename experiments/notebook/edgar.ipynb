{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as _hex_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_scheduled = _hex_json.loads(\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_user_email = _hex_json.loads('\"example-user@example.com\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_user_attributes = _hex_json.loads(\"{}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_run_context = _hex_json.loads('\"logic\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_timezone = _hex_json.loads('\"UTC\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_project_id = _hex_json.loads('\"01993942-1389-7000-8cd5-5d74ea1c200b\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_project_name = _hex_json.loads('\"SEC EDGAR Data Loader\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_status = _hex_json.loads('\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_categories = _hex_json.loads(\"[]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_color_palette = _hex_json.loads(\n",
    "    '[\"#4C78A8\",\"#F58518\",\"#E45756\",\"#72B7B2\",\"#54A24B\",\"#EECA3B\",\"#B279A2\",\"#FF9DA6\",\"#9D755D\",\"#BAB0AC\"]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mTry `uv pip` instead for faster package installs:\n",
       "\u001b[32m!uv pip install litellm numpy pandas python-dotenv requests\n",
       "\u001b[0mLearn more: https://learn.hex.tech/docs/explore-data/projects/environment-configuration/using-packages\n",
       "\n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
       "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install litellm>=1.77.3 numpy>=2.3.3 pandas>=2.3.2 python-dotenv>=1.1.1 requests>=2.32.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[92m16:25:14 - LiteLLM:DEBUG\u001b[0m: http_handler.py:580 - Using AiohttpTransport...\n",
       "\u001b[92m16:25:14 - LiteLLM:DEBUG\u001b[0m: http_handler.py:637 - Creating AiohttpTransport...\n",
       "\u001b[92m16:25:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:180 - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'\n",
       "\u001b[92m16:25:15 - LiteLLM:DEBUG\u001b[0m: http_handler.py:580 - Using AiohttpTransport...\n",
       "\u001b[92m16:25:15 - LiteLLM:DEBUG\u001b[0m: http_handler.py:637 - Creating AiohttpTransport...\n",
       "\u001b[92m16:25:15 - LiteLLM:DEBUG\u001b[0m: http_handler.py:580 - Using AiohttpTransport...\n",
       "\u001b[92m16:25:15 - LiteLLM:DEBUG\u001b[0m: http_handler.py:637 - Creating AiohttpTransport...\n",
       "\u001b[92m16:25:16 - LiteLLM:DEBUG\u001b[0m: http_handler.py:580 - Using AiohttpTransport...\n",
       "\u001b[92m16:25:16 - LiteLLM:DEBUG\u001b[0m: http_handler.py:637 - Creating AiohttpTransport...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from litellm import embedding\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheService:\n",
    "    \"\"\"Service for managing cache of processed company data\"\"\"\n",
    "\n",
    "    def __init__(self, cache_file: str = \"cache.csv\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = {}\n",
    "        self._load_cache()\n",
    "\n",
    "    def _load_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                df = pd.read_csv(self.cache_file, dtype={\"cik\": str, \"padded_cik\": str, \"sic\": str})\n",
    "\n",
    "                if \"embedded_description\" in df.columns:\n",
    "                    df[\"embedded_description\"] = df[\"embedded_description\"].apply(\n",
    "                        lambda x: ast.literal_eval(x) if pd.notna(x) and x != \"\" else None\n",
    "                    )\n",
    "\n",
    "                for _, row in df.iterrows():\n",
    "                    self.cache[str(row[\"cik\"])] = row.to_dict()\n",
    "\n",
    "                logger.info(f\"Loaded cache with {len(self.cache)} entries\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading cache from CSV: {e}\")\n",
    "                self.cache = {}\n",
    "        else:\n",
    "            self.cache = {}\n",
    "            logger.info(\"Starting with empty cache\")\n",
    "\n",
    "    def get(self, cik: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get cached data for a CIK\n",
    "\n",
    "        Args:\n",
    "            cik: Company CIK\n",
    "\n",
    "        Returns:\n",
    "            Cached company data or None if not found/invalid\n",
    "        \"\"\"\n",
    "        cik_key = str(cik)\n",
    "        if cik_key in self.cache:\n",
    "            cached_data = self.cache[cik_key]\n",
    "            if isinstance(cached_data, dict) and \"cik\" in cached_data:\n",
    "                # Clean up NaN values from pandas\n",
    "                cleaned_data = {}\n",
    "                for k, v in cached_data.items():\n",
    "                    if isinstance(v, (list, np.ndarray)):\n",
    "                        cleaned_data[k] = v\n",
    "                    elif pd.isna(v):\n",
    "                        cleaned_data[k] = None\n",
    "                    else:\n",
    "                        cleaned_data[k] = v\n",
    "                return cleaned_data\n",
    "        return None\n",
    "\n",
    "    def set(self, cik: str, data: Dict[str, Any]):\n",
    "        \"\"\"Set cached data for a CIK\n",
    "\n",
    "        Args:\n",
    "            cik: Company CIK\n",
    "            data: Company data to cache\n",
    "        \"\"\"\n",
    "        cik_key = str(cik)\n",
    "        self.cache[cik_key] = data\n",
    "        self.save()\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Save the cache to CSV file\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)\n",
    "\n",
    "            if self.cache:\n",
    "                df = pd.DataFrame.from_dict(self.cache, orient=\"index\")\n",
    "\n",
    "                if \"embedded_description\" in df.columns:\n",
    "                    df[\"embedded_description\"] = df[\"embedded_description\"].apply(\n",
    "                        lambda x: str(x) if x is not None else None\n",
    "                    )\n",
    "\n",
    "                # Save to CSV\n",
    "                temp_file = self.cache_file + \".tmp\"\n",
    "                df.to_csv(temp_file, index=False)\n",
    "                os.replace(temp_file, self.cache_file)\n",
    "\n",
    "                logger.debug(f\"Cache saved with {len(self.cache)} entries to {self.cache_file}\")\n",
    "            else:\n",
    "                pd.DataFrame().to_csv(self.cache_file, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save cache to {self.cache_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARExtractor:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Company Similarity Analysis contact@research.edu\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Host\": \"data.sec.gov\",\n",
    "        }\n",
    "        self.base_url = \"https://data.sec.gov/api/xbrl\"\n",
    "        self.rate_limit = 0.1  # 10 requests per second\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "    def get_total_revenue(self, company_facts: dict) -> float:\n",
    "        \"\"\"Get total revenue from company facts\"\"\"\n",
    "        possible_tags = [\n",
    "            \"SalesRevenueNet\",\n",
    "            \"Revenues\",\n",
    "            \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
    "            \"TotalRevenues\",\n",
    "        ]\n",
    "\n",
    "        if not company_facts or \"facts\" not in company_facts:\n",
    "            raise Exception(\"Invalid company facts\")\n",
    "\n",
    "        us_gaap = company_facts[\"facts\"][\"us-gaap\"]\n",
    "\n",
    "        annual_revenue = None\n",
    "        for tag in possible_tags:\n",
    "            if tag in us_gaap:\n",
    "                historical_revenue = us_gaap.get(tag, {}).get(\"units\", {}).get(\"USD\", [])\n",
    "                if historical_revenue:\n",
    "                    annual_data = [data for data in historical_revenue if data.get(\"form\") == \"10-K\"]\n",
    "                    if annual_data:\n",
    "                        annual_revenue = annual_data[-1][\"val\"]\n",
    "                        break\n",
    "\n",
    "        if not annual_revenue:\n",
    "            raise Exception(\"No annual revenue found\")\n",
    "\n",
    "        return annual_revenue\n",
    "\n",
    "    def get_company_sic(self, company_submissions: dict) -> str:\n",
    "        \"\"\"Get company SIC from company submissions\"\"\"\n",
    "        if not company_submissions or \"sic\" not in company_submissions:\n",
    "            raise Exception(\"Invalid company submissions\")\n",
    "\n",
    "        return company_submissions[\"sic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARService:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Company Similarity Analysis contact@research.edu\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Host\": \"data.sec.gov\",\n",
    "        }\n",
    "        self.base_url = \"https://data.sec.gov/api/xbrl\"\n",
    "        self.rate_limit = 0.1  # 10 requests per second\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "    def get_company_facts(self, cik: str) -> Dict:\n",
    "        \"\"\"Get company facts from EDGAR API\"\"\"\n",
    "        padded_cik = str(cik).zfill(10)\n",
    "        url = f\"{self.base_url}/companyfacts/CIK{padded_cik}.json\"\n",
    "\n",
    "        try:\n",
    "            time.sleep(self.rate_limit)\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            raise Exception(f\"Failed to fetch CIK {cik}: {e}\") from e\n",
    "\n",
    "    def get_company_submissions(self, cik: str) -> str:\n",
    "        \"\"\"Get company description from EDGAR API\"\"\"\n",
    "        padded_cik = str(cik).zfill(10)\n",
    "        url = f\"https://data.sec.gov/submissions/CIK{padded_cik}.json\"\n",
    "\n",
    "        try:\n",
    "            time.sleep(self.rate_limit)\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            raise Exception(f\"Failed to fetch CIK {cik}: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorService:\n",
    "    \"\"\"Service for managing error tracking in CSV format\"\"\"\n",
    "\n",
    "    def __init__(self, error_file: str = \"errors.csv\"):\n",
    "        self.error_file = error_file\n",
    "        self.errors = []\n",
    "        self._load_errors()\n",
    "\n",
    "    def _load_errors(self):\n",
    "        \"\"\"Load existing errors from CSV if file exists\"\"\"\n",
    "        if os.path.exists(self.error_file):\n",
    "            try:\n",
    "                df = pd.read_csv(self.error_file, dtype={\"cik\": str})\n",
    "                self.errors = df.to_dict(\"records\")\n",
    "            except Exception:\n",
    "                self.errors = []\n",
    "        else:\n",
    "            self.errors = []\n",
    "            self._save_to_csv()\n",
    "\n",
    "    def add_error(self, company_name: str, cik: str, error_type: str, error_message: str = None):\n",
    "        \"\"\"Add an error and immediately save to CSV\n",
    "\n",
    "        Args:\n",
    "            company_name: Name of the company\n",
    "            cik: Company CIK\n",
    "            error_type: Type of error (e.g., 'revenue_extraction', 'fmp_info_extraction')\n",
    "            error_message: Optional detailed error message\n",
    "        \"\"\"\n",
    "        error_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"company_name\": company_name,\n",
    "            \"cik\": str(cik),\n",
    "            \"error_type\": error_type,\n",
    "            \"error_message\": error_message or \"\",\n",
    "        }\n",
    "\n",
    "        self.errors.append(error_entry)\n",
    "        self._save_to_csv()\n",
    "\n",
    "    def _save_to_csv(self):\n",
    "        \"\"\"Save all errors to CSV\"\"\"\n",
    "        try:\n",
    "            if self.errors:\n",
    "                df = pd.DataFrame(self.errors)\n",
    "            else:\n",
    "                df = pd.DataFrame(columns=[\"timestamp\", \"company_name\", \"cik\", \"error_type\", \"error_message\"])\n",
    "\n",
    "            temp_file = self.error_file + \".tmp\"\n",
    "            df.to_csv(temp_file, index=False)\n",
    "            os.replace(temp_file, self.error_file)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def remove_errors_by_cik(self, cik: str):\n",
    "        \"\"\"Remove all errors for a specific CIK and save to CSV\n",
    "\n",
    "        Args:\n",
    "            cik: Company CIK to remove errors for\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cik_str = str(cik)\n",
    "            original_count = len(self.errors)\n",
    "            self.errors = [e for e in self.errors if str(e.get(\"cik\", \"\")) != cik_str]\n",
    "\n",
    "            if len(self.errors) < original_count:\n",
    "                self._save_to_csv()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def get_error_count(self) -> int:\n",
    "        \"\"\"Get the total number of errors\"\"\"\n",
    "        return len(self.errors)\n",
    "\n",
    "    def get_errors_by_type(self, error_type: str) -> List[dict]:\n",
    "        \"\"\"Get all errors of a specific type\"\"\"\n",
    "        return [e for e in self.errors if e[\"error_type\"] == error_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMPService:\n",
    "    \"\"\"Financial Modeling Prep API Service\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        \"\"\"Initialize FMP Service with API key\n",
    "\n",
    "        Args:\n",
    "            api_key: FMP API key. If not provided, will look for FINANCIAL_MODELLING_GROUP_API_KEY env var\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"FINANCIAL_MODELLING_GROUP_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"FMP API key not provided and FINANCIAL_MODELLING_GROUP_API_KEY not found in environment\")\n",
    "\n",
    "        self.base_url = \"https://financialmodelingprep.com\"\n",
    "        self.rate_limit = 0.2\n",
    "\n",
    "    def get_company_info(self, cik: str) -> Dict:\n",
    "        \"\"\"Get company information from FMP API using CIK\n",
    "\n",
    "        Args:\n",
    "            cik: Company CIK (can be with or without leading zeros)\n",
    "\n",
    "        Returns:\n",
    "            dict: Company info with description, marketCap, and fullTimeEmployees\n",
    "        \"\"\"\n",
    "        cik_no_zeros = str(int(cik))\n",
    "        url = f\"{self.base_url}/stable/profile-cik?cik={cik_no_zeros}&apikey={self.api_key}\"\n",
    "\n",
    "        try:\n",
    "            time.sleep(self.rate_limit)\n",
    "\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if data and len(data) > 0:\n",
    "                company_profile = data[0]\n",
    "                return {\n",
    "                    \"description\": company_profile.get(\"description\", None),\n",
    "                    \"market_cap\": company_profile.get(\"marketCap\", None),\n",
    "                    \"full_time_employees\": company_profile.get(\"fullTimeEmployees\", None),\n",
    "                }\n",
    "            else:\n",
    "                raise Exception(f\"No data returned from FMP API for CIK {cik_no_zeros}\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            raise Exception(f\"Failed to fetch FMP profile: {e}\") from e\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing FMP response: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeRevenue:\n",
    "    def __init__(self, cik_to_revenue: dict[int, float], number_of_buckets: int = 10):\n",
    "        self.number_of_buckets = number_of_buckets\n",
    "        self.cik_to_revenue = cik_to_revenue\n",
    "        self.revenues = self._get_revenues()\n",
    "        self.buckets = self._create_buckets()\n",
    "        for cik, revenue in self.cik_to_revenue.items():\n",
    "            idx = self._get_bucket_index(revenue)\n",
    "            self.buckets[idx].append(cik)\n",
    "\n",
    "    def normalize_all(self) -> list[int]:\n",
    "        return [self.normalize(cik) for cik in self.cik_to_revenue.keys()]\n",
    "\n",
    "    def normalize(self, cik: int) -> int:\n",
    "        return self._get_bucket_index(self.cik_to_revenue[cik])\n",
    "\n",
    "    def _create_buckets(self) -> list[list[int]]:\n",
    "        return [[] for _ in range(self.number_of_buckets)]\n",
    "\n",
    "    def _get_revenues(self) -> list[float]:\n",
    "        return list(self.cik_to_revenue.values())\n",
    "\n",
    "    def _get_min_revenue(self) -> float:\n",
    "        return min(self.revenues)\n",
    "\n",
    "    def _get_max_revenue(self) -> float:\n",
    "        return max(self.revenues)\n",
    "\n",
    "    def _get_bucket_size(self) -> float:\n",
    "        return (\n",
    "            (self._get_max_revenue() - self._get_min_revenue()) / self.number_of_buckets\n",
    "            if self.number_of_buckets\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "    def _get_bucket_index(self, revenue: float) -> int:\n",
    "        size = self._get_bucket_size()\n",
    "        if size == 0:\n",
    "            return 0\n",
    "        if revenue >= self._get_max_revenue():\n",
    "            return self.number_of_buckets - 1\n",
    "        idx = int((revenue - self._get_min_revenue()) / size)\n",
    "        if idx < 0:\n",
    "            idx = 0\n",
    "        if idx >= self.number_of_buckets:\n",
    "            idx = self.number_of_buckets - 1\n",
    "        return idx\n",
    "\n",
    "    def _get_bucket_range(self, bucket_index: int) -> tuple[float, float]:\n",
    "        size = self._get_bucket_size()\n",
    "        low = self._get_min_revenue() + bucket_index * size\n",
    "        high = (\n",
    "            self._get_min_revenue() + (bucket_index + 1) * size\n",
    "            if bucket_index < self.number_of_buckets - 1\n",
    "            else self._get_max_revenue()\n",
    "        )\n",
    "        return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_buckets_json(normalized_revenue, filepath: str = \"buckets.json\"):\n",
    "    \"\"\"Save revenue buckets to JSON file\n",
    "\n",
    "    Args:\n",
    "        normalized_revenue: NormalizeRevenue instance with bucket data\n",
    "        filepath: Path to save the JSON file\n",
    "\n",
    "    Returns:\n",
    "        Number of buckets saved\n",
    "    \"\"\"\n",
    "    buckets_data = []\n",
    "    for bucket_idx in range(normalized_revenue.number_of_buckets):\n",
    "        start, end = normalized_revenue._get_bucket_range(bucket_idx)\n",
    "        cik_values = [cik for cik in normalized_revenue.buckets[bucket_idx]]\n",
    "\n",
    "        buckets_data.append({\"bucket\": bucket_idx, \"range\": [start, end], \"cik_values\": cik_values})\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(buckets_data, f, indent=2)\n",
    "\n",
    "    return len(buckets_data)\n",
    "\n",
    "\n",
    "def create_cik_to_revenue_dict(companies_data: List[Dict[str, Any]]) -> Dict[int, float]:\n",
    "    \"\"\"Create a dictionary mapping CIK to revenue from companies data\n",
    "\n",
    "    Args:\n",
    "        companies_data: List of company dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping CIK to revenue\n",
    "    \"\"\"\n",
    "    cik_to_revenue = {}\n",
    "    for company in companies_data:\n",
    "        if company[\"total_revenue\"] is not None:\n",
    "            cik_to_revenue[company[\"cik\"]] = company[\"total_revenue\"]\n",
    "    return cik_to_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar_service = EDGARService()\n",
    "fmp_service = FMPService()\n",
    "extractor = EDGARExtractor()\n",
    "cache_service = CacheService()\n",
    "error_service = ErrorService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.hex.export+parquet": "{\"success\":true,\"exportKey\":\"0198b853-508d-7001-bdf5-37fb86d95814/01993942-1389-7000-8cd5-5d74ea1c200b/exports/01997765-83f4-7115-b85c-dec249cd0e6f\"}",
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>entityName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1750</td>\n",
       "      <td>AAR CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800</td>\n",
       "      <td>ABBOTT LABORATORIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1961</td>\n",
       "      <td>WORLDS INC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2034</td>\n",
       "      <td>ACETO CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2098</td>\n",
       "      <td>ACME UNITED CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2178</td>\n",
       "      <td>ADAMS RESOURCES &amp; ENERGY, INC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2186</td>\n",
       "      <td>BK TECHNOLOGIES CORPORATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2488</td>\n",
       "      <td>ADVANCED MICRO DEVICES, INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2491</td>\n",
       "      <td>BALLY TECHNOLOGIES, INC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2969</td>\n",
       "      <td>AIR PRODUCTS AND CHEMICALS, INC.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_cik = pd.read_csv(\"companyfacts_unique.csv\")\n",
    "company_cik.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing companies:   0%|          | 34/13912 [00:11<1:58:01,  1.96it/s]\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:26:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:26:19 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:26:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   0%|          | 38/13912 [00:13<1:50:33,  2.09it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:26:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   0%|          | 44/13912 [00:15<1:23:14,  2.78it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   0%|          | 63/13912 [00:21<56:22,  4.09it/s]  \u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:26:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:26:29 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:26:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   0%|          | 68/13912 [00:23<1:01:44,  3.74it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   1%|          | 160/13912 [00:52<1:38:03,  2.34it/s]\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:26:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:26:59 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   1%|          | 161/13912 [00:53<2:34:48,  1.48it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   1%|          | 173/13912 [00:58<1:49:41,  2.09it/s]\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:27:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:27:06 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:27:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   1%|▏         | 176/13912 [01:00<1:49:07,  2.10it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   2%|▏         | 248/13912 [01:29<1:41:39,  2.24it/s]\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:27:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   2%|▏         | 249/13912 [01:34<4:31:31,  1.19s/it]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   2%|▏         | 255/13912 [01:36<2:29:42,  1.52it/s]\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:27:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   2%|▏         | 257/13912 [01:38<2:26:58,  1.55it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   2%|▏         | 262/13912 [01:39<1:30:57,  2.50it/s]\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:27:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:27:46 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:27:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   2%|▏         | 263/13912 [01:40<2:03:06,  1.85it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   2%|▏         | 283/13912 [01:50<2:56:58,  1.28it/s]\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:27:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:27:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:27:58 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:27:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   2%|▏         | 286/13912 [01:52<2:31:16,  1.50it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   3%|▎         | 461/13912 [02:47<50:30,  4.44it/s]\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:28:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   3%|▎         | 464/13912 [02:48<1:00:43,  3.69it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   3%|▎         | 479/13912 [02:50<34:48,  6.43it/s]\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:28:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   3%|▎         | 483/13912 [02:51<40:35,  5.51it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   4%|▍         | 578/13912 [03:37<54:39,  4.07it/s]  \u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:29:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   4%|▍         | 579/13912 [03:38<1:16:08,  2.92it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   4%|▍         | 582/13912 [03:38<59:51,  3.71it/s]  \u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:29:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:29:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:29:46 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:29:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   4%|▍         | 587/13912 [03:40<1:12:17,  3.07it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   4%|▍         | 606/13912 [03:47<1:19:23,  2.79it/s]\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:29:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:29:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:29:55 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:29:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   4%|▍         | 608/13912 [03:49<1:39:23,  2.23it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   5%|▍         | 688/13912 [04:20<1:18:54,  2.79it/s]\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:30:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:30:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:30:27 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:30:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   5%|▍         | 689/13912 [04:21<1:44:13,  2.11it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   5%|▍         | 690/13912 [04:22<2:00:35,  1.83it/s]\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:30:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:30:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:30:29 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:30:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   5%|▍         | 691/13912 [04:23<2:41:41,  1.36it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   5%|▌         | 710/13912 [04:28<58:24,  3.77it/s]  \u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:30:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   5%|▌         | 711/13912 [04:29<1:20:40,  2.73it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   6%|▌         | 776/13912 [04:55<1:55:30,  1.90it/s]\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:31:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:31:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:31:02 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:31:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   6%|▌         | 779/13912 [04:56<1:44:28,  2.10it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   6%|▌         | 827/13912 [05:12<1:45:12,  2.07it/s]\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:31:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   6%|▌         | 830/13912 [05:13<1:37:42,  2.23it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   6%|▋         | 879/13912 [05:37<1:06:32,  3.26it/s]\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:31:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   6%|▋         | 880/13912 [05:38<1:28:53,  2.44it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   7%|▋         | 1030/13912 [06:43<2:10:40,  1.64it/s]\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:32:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:32:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:32:51 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:32:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   7%|▋         | 1031/13912 [06:45<3:07:31,  1.14it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   7%|▋         | 1032/13912 [06:45<2:42:43,  1.32it/s]\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:32:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:32:53 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:32:53 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:32:53 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   7%|▋         | 1035/13912 [06:47<2:16:42,  1.57it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   7%|▋         | 1040/13912 [06:50<2:27:09,  1.46it/s]\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:32:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   7%|▋         | 1041/13912 [06:52<2:47:47,  1.28it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   8%|▊         | 1048/13912 [06:55<2:00:14,  1.78it/s]\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:33:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:33:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:33:02 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:33:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   8%|▊         | 1049/13912 [06:56<2:34:52,  1.38it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   8%|▊         | 1068/13912 [07:06<1:34:59,  2.25it/s]\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:33:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   8%|▊         | 1072/13912 [07:16<5:34:14,  1.56s/it]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   8%|▊         | 1129/13912 [07:48<6:50:29,  1.93s/it]\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:33:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   8%|▊         | 1131/13912 [07:49<4:56:09,  1.39s/it]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   8%|▊         | 1158/13912 [08:02<1:42:42,  2.07it/s]\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:34:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   8%|▊         | 1159/13912 [08:03<2:08:44,  1.65it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   9%|▊         | 1194/13912 [08:14<1:21:22,  2.60it/s]\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:34:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:34:30 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:34:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   9%|▊         | 1195/13912 [08:24<8:09:27,  2.31s/it]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   9%|▉         | 1254/13912 [08:45<1:38:46,  2.14it/s]\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:34:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:34:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:34:54 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:34:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   9%|▉         | 1255/13912 [08:48<4:03:09,  1.15s/it]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:   9%|▉         | 1268/13912 [08:53<1:51:21,  1.89it/s]\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:35:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:   9%|▉         | 1269/13912 [08:54<2:18:21,  1.52it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  10%|█         | 1431/13912 [10:05<1:29:34,  2.32it/s]\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:36:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  10%|█         | 1433/13912 [10:07<1:37:59,  2.12it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  11%|█         | 1490/13912 [10:29<1:20:41,  2.57it/s]\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  11%|█         | 1491/13912 [10:30<1:44:42,  1.98it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  11%|█         | 1513/13912 [10:36<57:34,  3.59it/s]\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  11%|█         | 1514/13912 [10:38<1:21:32,  2.53it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  11%|█         | 1563/13912 [10:59<1:03:47,  3.23it/s]\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  11%|█▏        | 1567/13912 [11:00<1:02:03,  3.32it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  11%|█▏        | 1575/13912 [11:05<1:39:50,  2.06it/s]\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  11%|█▏        | 1576/13912 [11:06<2:11:57,  1.56it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  11%|█▏        | 1587/13912 [11:09<1:06:47,  3.08it/s]\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  11%|█▏        | 1588/13912 [11:10<1:29:09,  2.30it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1613/13912 [11:23<1:45:29,  1.94it/s]\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  12%|█▏        | 1614/13912 [11:24<2:13:09,  1.54it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1620/13912 [11:27<1:38:06,  2.09it/s]\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  12%|█▏        | 1621/13912 [11:28<1:56:51,  1.75it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1638/13912 [11:35<1:11:02,  2.88it/s]\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  12%|█▏        | 1641/13912 [11:36<1:12:18,  2.83it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1674/13912 [11:44<40:09,  5.08it/s]\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  12%|█▏        | 1675/13912 [11:45<1:02:06,  3.28it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1683/13912 [11:47<45:54,  4.44it/s]\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  12%|█▏        | 1686/13912 [11:48<53:25,  3.81it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1696/13912 [11:52<1:28:53,  2.29it/s]\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:37:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  12%|█▏        | 1697/13912 [11:53<1:53:08,  1.80it/s]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1715/13912 [12:08<1:56:08,  1.75it/s]\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mRequest to litellm:\u001b[0m\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \u001b[92mlitellm.embedding(model='openai/text-embedding-3-small', input=None)\u001b[0m\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - \n",
       "\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: utils.py:359 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:476 - self.optional_params: {}\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:941 - \u001b[92m\n",
       "\n",
       "POST Request Sent from LiteLLM:\n",
       "curl -X POST \\\n",
       "https://api.openai.com/v1 \\\n",
       "-d '{'model': 'text-embedding-3-small', 'input': None}'\n",
       "\u001b[0m\n",
       "\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1014 - RAW RESPONSE:\n",
       "Error code: 400 - {'error': {'message': \"'input' is a required property\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
       "\n",
       "\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2302 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
       "\u001b[92m16:38:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2482 - Logging Details LiteLLM-Failure Call: []\n",
       "Processing companies:  12%|█▏        | 1716/13912 [12:15<7:38:02,  2.25s/it]\n",
       "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
       "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
       "\n",
       "Processing companies:  12%|█▏        | 1720/13912 [12:17<3:28:50,  1.03s/it]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_data = []\n",
    "\n",
    "\n",
    "for _index, (cik, company_name) in tqdm(\n",
    "    enumerate(zip(company_cik[\"cik\"], company_cik[\"entityName\"], strict=False)),\n",
    "    total=len(company_cik),\n",
    "    desc=\"Processing companies\"\n",
    "):\n",
    "    padded_cik = str(cik).zfill(10)\n",
    "\n",
    "    cached_data = cache_service.get(cik)\n",
    "    if cached_data:\n",
    "        companies_data.append(cached_data)\n",
    "        continue\n",
    "\n",
    "    company_data = {\n",
    "        \"cik\": cik,\n",
    "        \"padded_cik\": padded_cik,\n",
    "        \"company_name\": company_name,\n",
    "        \"description\": None,\n",
    "        \"embedded_description\": None,\n",
    "        \"total_revenue\": None,\n",
    "        \"sic\": None,\n",
    "        \"market_cap\": None,\n",
    "        \"full_time_employees\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        company_facts = edgar_service.get_company_facts(cik)\n",
    "        company_submissions = edgar_service.get_company_submissions(cik)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching data for {company_name}: {e}\")\n",
    "        error_service.add_error(company_name, cik, \"data_fetch\", str(e))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        total_revenue = extractor.get_total_revenue(company_facts)\n",
    "        company_data[\"total_revenue\"] = total_revenue\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting total revenue for {company_name}: {e}\")\n",
    "        error_service.add_error(company_name, cik, \"revenue_extraction\", str(e))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        sic = extractor.get_company_sic(company_submissions)\n",
    "        company_data[\"sic\"] = sic\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting SIC for {company_name}: {e}\")\n",
    "        error_service.add_error(company_name, cik, \"sic_extraction\", str(e))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        fmp_info = fmp_service.get_company_info(cik)\n",
    "        company_data[\"description\"] = fmp_info[\"description\"]\n",
    "        company_data[\"market_cap\"] = fmp_info[\"market_cap\"]\n",
    "        company_data[\"full_time_employees\"] = fmp_info[\"full_time_employees\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting FMP info for {company_name}: {e}\")\n",
    "        error_service.add_error(company_name, cik, \"fmp_info_extraction\", str(e))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        company_data[\"embedded_description\"] = embedding(\n",
    "            model=\"openai/text-embedding-3-small\", input=company_data[\"description\"]\n",
    "        )[\"data\"][0][\"embedding\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error embedding description for {company_name}: {e}\")\n",
    "        error_service.add_error(company_name, cik, \"embedding_description\", str(e))\n",
    "        continue\n",
    "\n",
    "    log_data = {k: v for k, v in company_data.items() if k != \"embedded_description\"}\n",
    "    logger.debug(f\"Company information: {json.dumps(log_data, indent=4)}\")\n",
    "\n",
    "    cache_service.set(cik, company_data)\n",
    "    companies_data.append(company_data)\n",
    "    error_service.remove_errors_by_cik(cik)\n",
    "\n",
    "\n",
    "cik_to_revenue = create_cik_to_revenue_dict(companies_data)\n",
    "\n",
    "if cik_to_revenue:\n",
    "    normalized_revenue = NormalizeRevenue(cik_to_revenue, number_of_buckets=10)\n",
    "    normalized_revenue.normalize_all()\n",
    "    save_buckets_json(normalized_revenue)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "logger.info(f\"Processing complete. Total errors: {error_service.get_error_count()}\")"
   ]
  }
 ],
 "metadata": {
  "hex_info": {
   "author": "Apinan Yogaratnam",
   "exported_date": "Fri Sep 26 2025 12:39:26 GMT+0000 (Coordinated Universal Time)",
   "project_id": "01993942-1389-7000-8cd5-5d74ea1c200b",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
